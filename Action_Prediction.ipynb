{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation de dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset textuel généré et sauvegardé sous 'robot_text_commands.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Listes de descriptions textuelles\n",
    "distances = [\"à 1 mètre\", \"à 2 mètres\", \"à 5 mètres\", \"proche du robot\"]\n",
    "positions = [\"devant le robot\", \"à gauche\", \"à droite\", \"derrière le robot\"]\n",
    "états = [\"aucune obstruction visible\", \"un obstacle est détecté\", \"le robot est bloqué\"]\n",
    "\n",
    "# Générer des commandes en fonction des situations\n",
    "def generate_textual_dataset(num_samples=1000):\n",
    "    data = []\n",
    "    for _ in range(num_samples):\n",
    "        # Générer une situation\n",
    "        situation = f\"Il y a {random.choice(distances)} {random.choice(positions)}. {random.choice(états)}.\"\n",
    "        \n",
    "        # Définir une commande en fonction d'une règle simple\n",
    "        if \"devant\" in situation and \"obstacle\" in situation:\n",
    "            commande = \"arreter\"\n",
    "        elif \"à gauche\" in situation and \"obstacle\" in situation:\n",
    "            commande = \"tourner\"\n",
    "        else:\n",
    "            commande = \"revenir\"\n",
    "\n",
    "        data.append({\"situation\": situation, \"commande\": commande})\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Génération du dataset\n",
    "dataset = generate_textual_dataset(1000)\n",
    "dataset.to_csv(\"robot_text_commands.csv\", index=False)\n",
    "print(\"Dataset textuel généré et sauvegardé sous 'robot_text_commands.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le dataset\n",
    "dataset = pd.read_csv(\"robot_text_commands.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraire les colonnes\n",
    "texts = dataset[\"situation\"].values\n",
    "labels = dataset[\"commande\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encodage des étiquettes\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "labels = encoder.fit_transform(labels)  # Convertir \"tourner\", \"arrêter\", \"revenir\" en 0, 1, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisation des textes\n",
    "tokenizer = Tokenizer(num_words=5000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding des séquences\n",
    "max_length = 20  # Longueur maximale des phrases\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Exemple de séquence tokenisée : {sequences[0]}\")\n",
    "print(f\"Exemple de séquence après padding : {padded_sequences[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres\n",
    "vocab_size = 5000\n",
    "embedding_dim = 100\n",
    "input_length = max_length\n",
    "\n",
    "# Modèle\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=input_length),\n",
    "    LSTM(64, return_sequences=False),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(3, activation='softmax')  # 3 classes de sortie\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilation\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînement\n",
    "model.fit(padded_sequences, labels, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
